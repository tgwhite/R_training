---
title: "Working with R"
author: "Taylor G. White"
date: "March 7, 2019"
output:
  html_document:
      df_print: tibble
      highlight: tango
      theme: readable
      toc: yes
      toc_depth: 3
      toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
urlcolor: blue    
---

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-96904809-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-96904809-2');
</script>

<script type='text/javascript'>
  
  $(document).ready(function () {
    $('.tocify-item').click(function(){
      console.log('sending pageview for', location.hash); 
      ga('send', 'pageview', '/' + location.hash);
    });
  });

  
</script>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi = 400)

# 
# html_document:
#     df_print: tibble
#     highlight: tango
#     theme: readable
#     toc: yes
#     toc_depth: 3
#   word_document:
#     fig_width: 8
#     toc: yes

```

## Tutorials

This tutorial includes both an aggregation of already available learning materials for the R language as well as real-world examples of R in use, aimed at business analysts. 

### Get Started 

_Download R and RStudio_

* [R](https://cloud.r-project.org/) 
* [RStudio](https://www.rstudio.com/products/rstudio/download/) 

_Syntax, objects, functions_

What are the basic rules of the language? How are objects created? How do functions work? What controls the flow of operations that are completed?

* [Good overall introduction](http://r-statistics.co/R-Tutorial.html) 
* [Data types](https://www.statmethods.net/input/datatypes.html)
* [Scripting style guide](http://adv-r.had.co.nz/Style.html)
 
### Exploratory Data Analysis (EDA)
Import data, take a look at it, compute basic summary statistics, filter/reshape your data, plot your data. 

* [Data import](https://www.datacamp.com/community/tutorials/r-data-import-tutorial)
* [Basic summary statistics](https://www.statmethods.net/stats/descriptives.html)
* [Filter/subset, data reshape, groupwise operations](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)
* [More dplyr examples](https://dplyr.tidyverse.org/)
* [Plotting with ggplot2](http://r-statistics.co/Complete-Ggplot2-Tutorial-Part1-With-R-Code.html)

### Advanced manipulations
Combine data from multiple sources for analysis. Clean your data for analysis, reshape it for different operations. 

* [Combine data with joins](https://stat545.com/bit001_dplyr-cheatsheet.html)
* [Tidying and reshaping data](http://garrettgman.github.io/tidying/)

### Open Data Sources

R has many packages for downloading data available publicly via API. 

The St. Louis Federal Reserve (FRED) has publicly available data on economic and social matters, primarily in the United States but also globally. This data can be downloaded across many time frames (depending on how far back the time series go) and on various geographic levels (national, state, county, MSA). 

The World Bank has extensive economic, political, and social data on most of the countries in the world, though not all countries have robust statistical services. R's `WDI` package enables one to programmatically download statistics like economic growth, population, exchange rates, energy consumption, etc. This data is reported annually. 

The U.S. census occurs every 10 years and includes economic and social data about life in the U.S. In between each census, there is the American Community Survey (ACS), which captures data on a subset of the U.S. population and makes these estimates available for stakeholders (in 1, 3, and 5 year spans). This data can cover areas as large as the entire U.S. or as small as a city-block (census block). 

Financial data can be downloaded from the `quantmod` package via open sources like FRED, Yahoo Finance, and if you have an account, Quandl. 

* [FRED](https://cran.r-project.org/web/packages/fredr/vignettes/fredr.html)
* [World Bank](https://cengel.github.io/gearup2016/worldbank.html)
* [US Census](https://cran.r-project.org/web/packages/tidycensus/tidycensus.pdf)
* [Quantmod financial data](https://www.quantmod.com/examples/intro/)

### Interactive Reports

R has features to produce reports programmatically. The great thing about this is that, with a click of a button, you can regenerate a report that will be based on whatever new data or inputs you choose, without having to spend time formatting Excel documents. This introduction was produced using RMarkdown, which is a way of combining ("knitting") text with R code. Documents can be made that show the underlying code or they can only show fully polished output for reports. 

Since RMarkdown allows one to create HTML files (which are read using web browsers), one can generate interactive plots for users to see without knowing any javascript or HTML. 

Last, R has a feature to create web applications using only R with a package called `shiny`. These allow users to expose their analytical work to the world for fully interactive use. 

* [RMarkdown](https://rmarkdown.rstudio.com/articles_intro.html)
* [HTML/Javascript widgets](http://www.htmlwidgets.org/showcase_leaflet.html)
* [Shiny Web Applications](https://shiny.rstudio.com/gallery/)


### Working with Excel in R

Excel isn't going away anytime soon, but that doesn't mean you can't get away from Excel to do tasks that would otherwise be hard to do without a scripting language. I consider Excel a good tool for sharing information with colleagues and presenting data -- it shouldn't be used for complex calculations, data manipulation, or plotting if it can be avoided. 

A common workflow that combines R and Excel is the following: 

1. There is some existing business data in Excel
2. Get data from Excel and import it to R
3. Do data manipulation, calculations, plotting in R
4. Export results back to Excel for your colleagues to use

There are many R packages that can help with this work, but you should be careful about which ones you choose because they have additional dependencies (code they rely on to work). 

`openxlsx` is my all-around favorite for working with excel in R. It allows one to import data from Excel and Export data and images back to Excel, using only [RTools](https://cran.r-project.org/bin/windows/Rtools/) as a dependency. If all you need to do is read Excel data in R, you should use something simple like the `readxl` package. 

There are other R packages that interface with Excel, but these can cause additional headaches. Packages like `xlsx` and `XLConnect` rely on Java to connect to R packages. Most machines have Java installed, but there are often issues with getting the R environment to find the proper java.exe file to use. You're better off to just avoid this pain if you can.

### Statistical models

These methods are for fitting a given model to a set of data. These models are good for explaining relationships between quantities of interest and can be used for prediction. Each model is built for a particular purpose and they will not perform well if the assumptions underlying the model are violated. 

* [Linear regression](https://www.statmethods.net/stats/regression.html) - Fit a linear model via ordinary least squares. 
* [Logistic models](https://stats.idre.ucla.edu/r/dae/logit-regression/) - Discrete events (e.g. something happened or didn't happen)
* [Poisson models](https://stats.idre.ucla.edu/r/dae/poisson-regression/) - Counting processes
* [Time series models](https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials) - Follow a single entity over time. Observations are correlated over time. 
* [panel data analysis](https://www.princeton.edu/~otorres/Panel101R.pdf) - for data sets that follow entities (e.g. people, companies, countries) over time. 
* [quantile regression](https://data.library.virginia.edu/getting-started-with-quantile-regression/) - OLS regression will find the conditional mean -- this method can find the conditional Nth percentile, e.g. 10% percentile, median, or 90th percentile. 
* [Advanced parametric models](https://stats.idre.ucla.edu/other/dae/)

### Machine learning

These methods can help explain unseen relationships in data that parametric models might not capture. These methods are better for prediction than explanation, and often outperform parametric models in terms of prediction. The downside is you might not be able to explain your results to stakeholders, as many machine learning methods are black boxes. 

* [Cluster analysis](https://www.statmethods.net/advstats/cluster.html) - create clusters of entities based on a host of variables
* [Regression and classification trees](https://www.statmethods.net/advstats/cart.html) - Split data into homogeneous groups and continue doing so until a split no longer produces meaningful differences. 
* [Neural networks](https://beckmw.wordpress.com/tag/neuralnet/)
* [Random forests](https://www.r-bloggers.com/how-to-implement-random-forests-in-r/) - In parallel, fit many regression trees on bootstrapped input datasets, using a subset of variables each time. Combine the results. 
* [Boosted trees](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html) - In sequence, fit many small regression trees (not "full" trees with many splits) on bootstrapped input datasets. Combine the results. 

## Learning R

R is said to have a steep learning curve, but one is well-rewarded once you get over some of its idiosyncracies. The best advice I have to learn R (and any other language) is to take a little time every day to practice, even if its just for 30 minutes in the morning. Second, use R to solve actual problems that you have. Many introductions out there show you how to do things you don't ever need to do. This doesn't help one commit techniques to memory. Instead, work on a real-life problem you have and use Google as your ally. Search "how to do [x] in R" and you'll be presented with many articles from people that have worked on the same problems. 

Aside from Google, your best ally to deal with problems you encounter is [StackOverflow](https://stackoverflow.com/), which is a programming help site. For statistical questions, go to [Cross Validated](https://stats.stackexchange.com/), which often has examples in R. 

Below are some additional gotchas and helpful information you should be aware of.

### Something unexpected is happening! 
* You ran a function and it is giving you strange output. Make sure that the `class()` of the object(s) you are operating on are what you expected and that it has the data you expect data.
* Inspect your data. Print it to the console, look at the top and bottom of it. Use plots to visualize your whole dataset in various ways. Make sure you aren't making incorrect assumptions about your data. 
* You changed your code in some way, and you are referring to something other than what you expected. Good and consistent naming conventions can help with these problems. 

```{r, eval = F}
# original code
oldthing = 1 

some_important_value = oldthing^2

# updated code
oldThing = 2

some_important_value = oldthing^2 # still returns 1 if you didn't clear your environment! 

```



### This code worked the first time but it isn't working now
You are likely modifying an object in place. This is usually bad and leads to strange errors. The value of the object `thing` is dependent on how many times you ran the `thing = thing + 2` line. This error is unlikely in this case, but it can happen with longer data prep code chunks. 

```{r, eval = F}
# This is modifying the object in place
thing = 1

thing = thing + 2
```

### I don't understand that error!
Stackoverflow and Google are your friends. Copy the whole error and paste it into google. See what comes up. If nothing comes up, copy just the first part that isn't particular to your piece of code and see if a search engine finds something. Chances are, you have made a mistake someone else has made many times before. 

### Nothing is working and I'm freaking out! 
Relax. Take a break. Walk away from the code for a bit and come back to it when you have regained composure. Chances are, frantically changing bits of code will lead to even more problems. The way to uncover bugs is to isolate the changes you make in order to systematically test each chunk for issues. Frantic changes will mean you could fix one problem and introduce many others, leading you to believe the whole piece of code is broken in some fundamental way. 

### I have a lot to do and don't know how to proceed! 
Think before you write any code. Go to a piece of paper or whiteboard and map out the problem you are working on. When you have a decent grasp, write code in a piecewise fashion, instead of trying to tackle everything at once. Small, manageable code pieces that work can be wrapped in functions and pieced together. If there is an error, it's easy to trace that error to a particular function, instead of having to debug a large piece of [spaghetti](https://en.wikipedia.org/wiki/Spaghetti_code) code. 

### Be careful with `factor` vectors
Factors are essentially vectors that have some sort of label, saved as an integer, treated as a discrete value. Some processes will coerce factors back to integer values, which then would allow for numeric operations. Make sure weirdness isn't happening because of factors.  


## Example 1 - Download stock prices, estimate beta

This example demonstrates how to get publicly available data to estimate betas for a set of stock tickers. 

_Steps_

* Get prices for multiple stocks: Chevron, ExxonMobile, First Solar Inc., GE, Tesla
* Get values of the S&P 500 
* Get values of the risk free interest rate
* Compute monthly returns for all of these series
* Combine data into a single dataset for analysis
* Subtract the risk free rate from asset and market returns
* Regress stock returns in excess of the risk free rate from market returns in excess of the risk free rate
* Extract betas from each model

```{r}
# Set up your script

# load libraries 
library(quantmod) # download data and other financial functions
library(tidyverse) # host of data handling and visualization functions
library(lubridate) # easier handling of dates
library(scales) # nice scales for ggplot2

# assign vectors of symbols to objects to be passed to quantmod's getSymbols() function
stock_tickers_vec = c('CVX', 'XOM', 'FSLR', 'GE', 'TSLA')

```

My intention is to run the same data preparation code for all of the symbols, so it wouldn't matter 
if I had to do this process for 1 or 10,000 stock tickers. That being said, I always write out my code for the single case to test each piece before wrapping it into a function. 

```{r}

# download daily data for the S&P500, from 2010 to the current date using Yahoo finance data
downloaded_data = getSymbols('^GSPC', auto.assign = F, src = 'yahoo', from = as.Date('2010-01-01'))

# take a look at the data
class(downloaded_data) # what type of object was downloaded?
head(downloaded_data)
tail(downloaded_data)

# get the class of all the columns. This will loop over each column and apply the class function to that column
sapply(downloaded_data, class) 

# Calculate monthly returns. Make sure you know what the underlying values are. In some cases, it's stock prices, in some cases it's the value of an index. 
monthly_returns = monthlyReturn(downloaded_data)
head(monthly_returns)

# convert the monthly return to a data.frame and add a date column using the index of the zoo/xts object
returns_df = data.frame(
  monthly_returns
) %>%
  rename(
    return = monthly.returns # make a prettier name for the monthly.returns column
  ) %>%
  mutate(
    # this will shift each date to the start of the month for simplicity of comparison
    date = floor_date(index(monthly_returns), 'month') 
  )

head(returns_df) # everything looks good.


```

Now I want to "wrap" this code into a function, so I can run the same code efficiently without having to repeat my code many times, which will cause errors and headaches. 

```{r}

# create the name for your function. Use a verb to describe it
# The only thing I need to do to get data for other tickers is to change the ticker that is fed through
# This is captured in the first  parameter defined in the get_monthly_returns_function
# The second parameter is from_date, which will determine how far back the data goes
# This parameter has a default value, set to January 1, 2010
# The third parameter is passed to getSymbols and indicates the source of the data. The default is set to yahoo

get_monthly_returns = function(ticker, from_date = as.Date('2010-01-01'), source = 'yahoo') {
  
  
  # FIRST CHANGE - sp500 is changed to ticker here, so other tickers can be fed through
  # I also changed the hard-coded from date to from_date, which allows for control of the 
  # downloaded dates
  downloaded_data = getSymbols(ticker, auto.assign = F, src = source, from = from_date)
  
  # SECOND CHANGE -  no need to take a look at the data within the function anymore
  
  # calculate monthly returns
  monthly_returns = monthlyReturn(downloaded_data)
  
  # convert the monthly return to a data.frame and add a date column using the index of the zoo/xts object
  returns_df = data.frame(
    monthly_returns
  ) %>%
    rename(
      return = monthly.returns # make a prettier name for the monthly.returns column
    ) %>%
    mutate(
      # this will shift each date to the start of the month for simplicity of comparison
      date = floor_date(index(monthly_returns), 'month'),
      
      # !!!!!
      # THIRD CHANGE -- I'm adding the ticker to the returns_df, to help keep track of 
      # which data I downloaded. This is super importantant, and helps make sure you know what data
      # you have. 
      ticker = ticker 
    )
  
  return(returns_df) # this is what the function gives us or "returns"
}

```

Did you notice the `%>%` operator in the code chunk above? That is called a pipe, and it helps make super clean code. It takes the results of one operation (function call) and "pipes" those results to the first argument of the next function. This allows code to be chained together linearly, from one step to the next. This stops a never ending series of parentheses. 

Now I have this nice function that can download daily values of data using yahoo finance. How can I use it?

Download data for one ticker, Tesla: 
```{r}
tesla_returns = get_monthly_returns('TSLA')
```

What about all the tickers? I want to download all of the data and "stack" the returns for efficient analysis. I will feed all of the stock tickers to this function I created and combine the results. 

```{r}

# lapply will loop over an input vector, feeding each element to the first argument of a function
# it will return results as a list, which is a handy type of object that can hold anything you like 

all_monthly_stock_returns = lapply(stock_tickers_vec, get_monthly_returns)

# all_monthly_stock_returns is a list. How can you access data within a list? 
class(all_monthly_stock_returns)

# look at the data in the first element
all_monthly_stock_returns[[1]] %>% head()

```
Each element of the `all_monthly_stock_returns` list is the `data.frame` returned by `get_monthly_returns`. How can this data be converted to a `data.frame` for analysis? 

```{r}
# bind rows will stack data.frame objects on top of each other. It accepts a series of data.frame objects or a list of data.frame objects
all_monthly_stock_returns_stacked = bind_rows(all_monthly_stock_returns)

# take a look at the stacked data
head(all_monthly_stock_returns_stacked)
tail(all_monthly_stock_returns_stacked)

# look at the counts for each ticker
table(all_monthly_stock_returns_stacked$ticker)

```
Now that we have all the stock data combined, we can do additional analysis on the data. I like to start by making plots of the data I have, which helps me understand the data I just downloaded, both in terms of how variables relate to each other, and in terms of accuracy (strangeness in data comes out very quickly when you plot all of it). 

```{r}

# ggplot is from the ggplot2 package, a (the) premier library for static plots

# the ggplot function sets up the "rules" for the plot. 
# The first argument is the data.frame to be used
# the next argument is aes(), which stands for aesthetics. This says which values should be passed
# on to other "geom_" functions, or layers of the plot for use
# here, date is the x variable, return is the y variable, and the data is plotted by group, using ticker

ggplot(data = all_monthly_stock_returns_stacked, aes(x = date, y = return, group = ticker)) + 
  geom_line()

```

This plot shows one line for each stock ticker that represents monthly returns. I don't see any weird discontinuities and the variability looks like what I would generally expect for stock returns. 

One thing about this plot is it's difficult to determine which line belonds to which stock. `ggplot2` makes it easy to plot groups of data, either by assinging different colors to values within a group, or by separating series into their own panels (or facets).


```{r}

# This version of the plot will group and color each line according to ticker
ggplot(all_monthly_stock_returns_stacked, aes(date, return, color = ticker)) + 
  geom_line()

# This version of the plot will split the lines into their own facets by ticker
ggplot(all_monthly_stock_returns_stacked, aes(date, return)) +
  facet_wrap(~ticker) + 
  geom_line()


```

This data makes sense to me - larger, more established companies have much less variation in monthly returns than TSLA and FSLR. Data is available at regular intervals. 

The next step is to get data for the S&P 500 and the risk free interest rate, which we will call the 13 week treasury bill rate. The same `get_monthly_returns` function can work for the S&P 500, and I'll use getSymbols() directly for the treasury rate with the St. Louis Federal Reserve ( [FRED](https://fred.stlouisfed.org/) ) as a source. To download FRED data, I went to FRED and typed in "treasury bill rate" in the seach bar and was presented with a set of options for time series that match that description. The first series is what I want: "3-Month Treasury Bill: Secondary Market Rate." Clicking the "monthly" option leads me to the following view: 

![fred_screencap](FRED Treasury Bill Screencap.png)
Note the url: https://fred.stlouisfed.org/series/TB3MS. The end of the URL (TB3MS) represents the identifier for the time series, which is what we will pass to quantmod to get data from FRED. As an aside, this is essentially how I got data from Yahoo finance for the S&P 500 and the 3 month treasury bill rate as well. I searched for these series and found the ticker representation for those values.

```{r}

# download monthly returns for the S&P 500
sp500 = get_monthly_returns('^GSPC')

# get the monthly risk free rate from FRED
rf_rate = getSymbols('TB3MS', src = 'FRED', auto.assign = F)
rf_rate_adj = data.frame(
  return = rf_rate/100, # the risk free rate is presented as a percent, needs to be in same terms as the returns
  date = index(rf_rate)
  ) %>% 
  rename(
    return = TB3MS
  )
tail(rf_rate_adj)

```

Now that I have all the data I need, I want to combine them for analysis. Since I want to run regressions of the form `(stock return - risk free rate) = alpha + beta(market_return - risk free rate) + error`, I need to have each stock return associated with the relevant risk free rate and market return for that month. We need to keep in mind that the stock returns are currently stacked on top of each other, so we can't simply run a regression with all that data -- we need to split the data by ticker and run each regression. There are efficient ways to do that in R, so it makes sense to keep those as a "long" `data.frame`. 

First, let's combine the S&P 500 data and the risk free rate. In this case, I want these values side by side (joined) by date. Right now, the data isn't quite set up to cleanly join these two `data.frame` objects - I have a date that can be used for a join (see the joins tutorial for an in-depth presentation) but I have columns that have the same name. If I were to simply join these, I'd have funky column names, e.g. `return` and `return.1` or something like that to identify which table the column came from. Further, `dplyr` join functions automatically will join values based on column names, so this would attempt to join by return and ticker, which would result in zero matches. 

```{r}

# I can do everything in one step: 
# get rid of columns I don't want
# rename columns to something different
# use a left join to pull all the data together
joined_comparators = rf_rate_adj %>%
  rename(
    rf_rate_return = return # rename the return column to rf_rate_return 
  ) %>% 
  left_join(
    # Pay attention to what is happening here
    # I am doing the column dropping and renaming steps WITHIN the left_join call
    # This means the join will happen on the result of these operations
    select(sp500, -ticker) %>% 
      rename(
        sp500_return = return
      )
  )

# Lets take a look at the result:
head(joined_comparators)
tail(joined_comparators)

# How do the series compare? 
ggplot(joined_comparators, aes(rf_rate_return, sp500_return)) + 
  geom_point() + 
  stat_smooth(method = 'lm') # add a linear model to summarize the relationship

```

Now we have our comparators ready, we can join this data onto the stock tickers using the date column. We don't have to worry about renaming any columns now -- the only common column is `date`. I use a left join again because that is the safest join -- it keeps all of the rows in the left hand side and only adds rows that match from the right. This allows one to see what data is potentially missing. 

Additional steps should be taken to ensure that your dates look right -- it's easy for missing values to go unnoticed in important places. Are there any dates in the comparators that aren't found in the stock returns? We already know that the series go out further for the FRED data (notice the `head` of the comparators) because `getSymbols` for FRED didn't respect the `from` argument. 


```{r}
# setdiff will subtract two sets and find the values in the left hand side that 
# aren't in the right side
# letters is a default R object that contains the letters of the alphabet. 
# letters[1:5] has the first 5 letters, letters[2:6] has 2-6. The difference is the first letter, 'a'
# because a is in the left side and not the right
setdiff(letters[1:5], letters[2:6])

# lets check our dates:
unique_stock_months = unique(all_monthly_stock_returns_stacked$date)
unique_comparator_months = unique(joined_comparators$date)

# all of the months in the stocks are found in the comparators! It's still possible for sp500 returns to be NA in some months though
setdiff(unique_stock_months, unique_comparator_months)

# lets join all of our data together and compute values net of the risk free rate! 
all_joined_analysis_data = left_join(all_monthly_stock_returns_stacked, joined_comparators) %>%
  mutate(
    return_net_rf = return - rf_rate_return,
    sp500_net_rf = sp500_return - rf_rate_return
  )
head(all_joined_analysis_data)
tail(all_joined_analysis_data)

```
Now we have all of our data joined together for analysis. You may have noticed that a fair amount of work was required simply to get to this point. The typical refrain from professional analysts is that around 90% of your time is spent gathering and cleaning data, with the remainder spent on analyzing/modeling/interpretation. That's what makes R so important - you can automate data preparation steps and you can do complex manipulations super efficiently (once you are over the learning curve) compared to tools like Excel.

Before we run our models, let's take one last look at our data for a sanity check. How do the series relate?

```{r}

# Market versus individual stocks
ggplot(all_joined_analysis_data, aes(sp500_return, return)) + 
  facet_wrap(~ticker, scales = 'free') + # allow each panel to have its own scale 
  geom_point() + 
  stat_smooth(method = 'lm') + 
  labs(
    x = 'Market Return', y = 'Stock Return', title = 'Market Returns vs. Individual Stocks'
  ) + 
  scale_y_continuous(labels = percent) + 
  scale_x_continuous(labels = percent)

# net of risk free rate
ggplot(all_joined_analysis_data, aes(sp500_net_rf, return_net_rf)) + 
  facet_wrap(~ticker, scales = 'free') + # allow each panel to have its own scale 
  geom_point() + 
  stat_smooth(method = 'lm') + 
  labs(
    x = 'Market Return', y = 'Stock Return', title = 'Market Returns vs. Individual Stocks',
    subtitle = 'Net of risk free returns'
  ) + 
  scale_y_continuous(labels = percent) + 
  scale_x_continuous(labels = percent) 

```
 
As theory predicts, there is a strong relationship between the market returns and individual stock returns net of the risk free rate. Let's estimate those betas. We want to run a model for each subset of data and extract model coefficients. 

As with before, I'll run some code for the single ticker case and then demonstrate how to do it for all tickers. 

```{r}

# subset to tesla
tesla_subset = filter(all_joined_analysis_data, ticker == 'TSLA')
head(tesla_subset)
tail(tesla_subset)

# fit linear model via OLS. To the left of ~ is the Y of the analysis, to the right are the Xs or predictors
# data specifies the data to be used
tesla_model = lm(return_net_rf ~ sp500_net_rf, data = tesla_subset)

# this will provide a summary of the model
summary(tesla_model)

```
The Estimate column shows the estimated coefficients for the intercept and the slope estimate (beta) for the market. We want to extract that value as our beta. 


```{r}
# this extracts just the coefficients
coef(tesla_model)

# you can also extract components from the summary function: 
names(summary(tesla_model)) # this is what you can get

# nice table of coefficient information with standard errors and t values
summary(tesla_model)$coefficients

# we just want the estimate for the market beta though. 

# this is a numeric vector, but as we saw above, it has names for each element. 
# USE THE NAMES TO EXTRACT DATA, NOT THE ELEMENT NUMBER
class(coef(tesla_model))

# using the name will make your life way easier in the long run. You can actually see what it is that you are doing and it's easier to catch mistakes
tesla_beta = coef(tesla_model)['sp500_net_rf']
print(tesla_beta)
```
Now we know how to get the beta for one stock, lets extract it for all of them in a loop! 

```{r}
# I'm down at the bottom of my script and I forgot the name of the vector
# where I stored all the stock tickers
# this function tells me what is in memory:
ls()

# loop over the stock tickers, subset the data, run the model, extract the beta
all_betas = lapply(stock_tickers_vec, function(the_ticker){
  ticker_sub = filter(all_joined_analysis_data, ticker == the_ticker)
  the_model = lm(return_net_rf ~ sp500_net_rf, data = ticker_sub)
  beta = coef(the_model)['sp500_net_rf']
  names(beta) = NULL # get rid of the sp500_net_rf column name
  return(beta)
})

# all betas is a list, let's give each element the name of the ticker it represents
names(all_betas) = stock_tickers_vec

print(all_betas)


```
Now you have your betas saved in a list and you can do what you want with them! Do these values make sense given what was seen in the scatterplot before? Now you can use these betas to build a portfolio. 
